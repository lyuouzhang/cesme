result_1$beta_hat
y_pred = Z%*%as.matrix(result_1$beta_hat)
as.matrix(result_1$beta_hat)
Z
dim(Z)
y_pred = t(Z)%*%result_1$beta_hat
y_pred
t(result_1$beta_hat)%*%Z
model_fit = function(X,Y,A,tol=1e-3,maxit=1e3,lam_g=NULL,lam_l=NULL,min_nz=NULL){
n = dim(Y)[2] ## Y: 1*n row vector.
d = dim(A)[2]
p = dim(A)[1]
# the tuning parameter of glasso is lam_g*sqrt(log(d)/n)
# if lam_g is NULL, use CVglasso to determine tuning parameter
if(!is.null(lam_g)){
lam_g = lam_g*sqrt(log(d)/n)
}
# the tuning parameter of glmnet is lam_l*sqrt(log(d)/n)
# if lam_l is NULL, use CV.glmnet to determine tuning parameter
if(!is.null(lam_l)){
lam_l = lam_l*sqrt(log(d)/n)
}
# the tuning parameter of glasso is lam_g*sqrt(log(d)/n)
# if lam_g is NULL, use CVglasso to determine tuning parameter
if(!is.null(lam_g)){
lam_g = lam_g*sqrt(log(d)/n)
}
# minimum number of non zero entries inCV.glmnet
# if min_nz is NULL, let min_nz=max(p/10,3)
if(!is.null(min_nz)){
min_nz = max(floor(p/10),3)
}
# EM procedure
aa=em1(X,A,tol,maxit,n,d,lam_g)
Sigma_Z=solve(aa$P)
Sigma_ZY=solve(t(A)%*%A)%*%t(A)%*%rowMeans(X%*%diag(as.vector(Y)))
Sigma_Y=mean(as.vector(Y)^2)
# lasso
res_lasso = lasso_fit(Sigma_Z,Sigma_ZY,Sigma_Y,n,lam=lam_l,min_nz)
beta_hat = res_lasso$beta_hat
# debiased lasso
result_1 = debiasd_lasso_fit(Sigma_Z,Sigma_ZY,beta_hat,res_lasso$sigma2e,n,d,lam=lam_l,min_nz)
# p-value
p_value_beta = result_1$p_value
# prediction of y
Z = solve(t(A)%*%A)%*%t(A)%*%X
y_pred = t(result_1$beta_hat)%*%Z
return(p_value_beta=p_value_beta,y_pred=y_pred)
}
set.seed(2023)
# A is p by d binary matrix
A = matrix(rbinom(p*d,size = 1,prob = 0.5),p,d)
# Z is d by n sparse matrix
temp <- erdos.renyi.game(n = d, p = 0.3) # 30% of the entries are 0.
adj <- as_adjacency_matrix(temp, sparse = F)
tot_not0 <- sum(adj == 1)
sample_U <- runif(tot_not0, 0.2, 0.5)
Cg <- adj
Cg[adj == 1] <- sample_U
S_star <- (Cg + t(Cg))/2
# to make C_star positive definite
diag(S_star) <- 0.5 - eigen(S_star)$values[d]
# S_star is the covariance matrix of Z
C_star = solve(S_star)
# beta is sparse
beta=c(rep(1,s),rep(0,d-s))
# snr1 controls snr for linear model
sigmaepsilon2 = t(beta)%*%S_star%*%beta/snr1/5
# snr2 controls snr for factor model
# Lambda is the covariance matrix of u
Lambda = diag(diag(A%*%S_star%*%t(A))/snr2)/5
# seed from i.rep
set.seed(2023+i.rep)
Z=t(mvrnorm(n,rep(0,d),S_star))
# u ~ N(0,Lambda)
u=t(mvrnorm(n,rep(0,p),Lambda))
# epsilon ~ N(0,sigmaepsilon2)
epsilon=t(rnorm(n,0,sigmaepsilon2))
X=A%*%Z+u
Y=t(t(Z)%*%beta)+epsilon
model_fit(X,Y,A)
model_fit = function(X,Y,A,tol=1e-3,maxit=1e3,lam_g=NULL,lam_l=NULL,min_nz=NULL){
n = dim(Y)[2] ## Y: 1*n row vector.
d = dim(A)[2]
p = dim(A)[1]
# the tuning parameter of glasso is lam_g*sqrt(log(d)/n)
# if lam_g is NULL, use CVglasso to determine tuning parameter
if(!is.null(lam_g)){
lam_g = lam_g*sqrt(log(d)/n)
}
# the tuning parameter of glmnet is lam_l*sqrt(log(d)/n)
# if lam_l is NULL, use CV.glmnet to determine tuning parameter
if(!is.null(lam_l)){
lam_l = lam_l*sqrt(log(d)/n)
}
# the tuning parameter of glasso is lam_g*sqrt(log(d)/n)
# if lam_g is NULL, use CVglasso to determine tuning parameter
if(!is.null(lam_g)){
lam_g = lam_g*sqrt(log(d)/n)
}
# minimum number of non zero entries inCV.glmnet
# if min_nz is NULL, let min_nz=max(p/10,3)
if(!is.null(min_nz)){
min_nz = max(floor(p/10),3)
}
# EM procedure
aa=em1(X,A,tol,maxit,n,d,lam_g)
Sigma_Z=solve(aa$P)
Sigma_ZY=solve(t(A)%*%A)%*%t(A)%*%rowMeans(X%*%diag(as.vector(Y)))
Sigma_Y=mean(as.vector(Y)^2)
# lasso
res_lasso = lasso_fit(Sigma_Z,Sigma_ZY,Sigma_Y,n,lam=lam_l,min_nz)
beta_hat = res_lasso$beta_hat
# debiased lasso
result_1 = debiasd_lasso_fit(Sigma_Z,Sigma_ZY,beta_hat,res_lasso$sigma2e,n,d,lam=lam_l,min_nz)
# p-value
p_value_beta = result_1$p_value
# prediction of y
Z = solve(t(A)%*%A)%*%t(A)%*%X
y_pred = t(result_1$beta_hat)%*%Z
return(list(p_value_beta=p_value_beta,y_pred=y_pred))
}
model_fit(X,Y,A)
n_fold=5
trainControl(method = "cv",number = n_fold)
library(caret)
install.packages("caret")
install.packages("caret")
library(caret)
trainControl(method = "cv",number = n_fold)
?trainControl
??trainControl
trainControl(method = "cv",number = n_fold)
install.packages("vctrs")
install.packages("vctrs")
library(caret)
trainControl(method = "cv",number = n_fold)
caret::trainControl(method = "cv",number = n_fold)
library(glasso)
library(MASS)
library(igraph)
library(msda)
library(lsei)
library(Matrix)
library(glmnet)
library(CVglasso)
library(caret)
install.packages("vctrs")
install.packages("vctrs")
install.packages("vctrs",type="binary")
install.packages("vctrs", type = "binary")
library(caret)
install.packages("vctrs")
library(glasso)
library(MASS)
library(igraph)
library(msda)
library(lsei)
library(Matrix)
library(glmnet)
library(CVglasso)
library(caret)
library(snow)
library(rlecuyer) # loads the random number generator needed for multi-core
library(doParallel) # replace parallel from snow
library(foreach)
#the R function which applies the EM algorithm
em1=function(X,A,tol,maxit,n,d,lam=NULL){
# Z is d by n matrix, A is p by d binary matrix
# the tuning parameter is lam*sqrt(log(d)/n)
# if lam is NULL, use CVglasso to determine tuning parameter
# if lam=0, use inverse instead of glasso
if(!is.null(lam)){
if(lam==0&&n<d){
stop("covariance matrix is not invertible for n<d, need graphical lasso for estimation")
}
}
p = dim(A)[1]
#initial value
#D0 = mean(eigen(var(t(X)))$value[-(1:d)])
#if (p==d) {D0=mean(eigen(var(t(X)))$value[d])}
# D initialized by average strength of var(X)
D0 = eigen(var(t(X)))$value[p-1]
D.temp=diag(rep(D0,p))
#P.temp = solve(t(A)%*%A)%*%t(A)%*%(var(t(X)) - diag(rep(D0,p)))%*%A%*%solve(t(A)%*%A)
#P.temp = solve(t(A)%*%A)%*%t(A)%*%(var(t(X)))%*%A%*%solve(t(A)%*%A)
#C.temp = solve(P.temp)
#C=diag(d)
# covariance matrix initialized by identity matrix
P.temp=diag(rep(1,d))
C.temp = solve(P.temp)
##now we begin the updating until convergence
k=0
dd=1
while(dd>=tol){
k=k+1
mu.temp=C.temp%*%t(A)%*%solve(A%*%C.temp%*%t(A)+D.temp)%*%X
Sigma.temp=C.temp-C.temp%*%t(A)%*%solve(A%*%C.temp%*%t(A)+D.temp)%*%A%*%C.temp
#mu.temp is p by n matrix
#update D
D=diag(rep(0,p))
for (i in 1:n){
D=D+(X[,i]%*%t(X[,i])-X[,i]%*%t(mu.temp[,i])%*%t(A)-A%*%mu.temp[,i]%*%t(X[,i])+A%*%mu.temp[,i]%*%t(A%*%mu.temp[,i])+A%*%Sigma.temp%*%t(A))
}
D=diag(abs(diag(D/n)))
#D=D/n
#update P
if(is.null(lam)){
res.glasso = CVglasso(S = Sigma.temp+mu.temp%*%t(mu.temp)/n,path = TRUE,crit.cv = "BIC")
P=res.glasso$Omega
C=res.glasso$Sigma
}else if(lam==0){
C = Sigma.temp+mu.temp%*%t(mu.temp)/n
P = solve(C)
}else{
res.glasso = glasso(s = Sigma.temp+mu.temp%*%t(mu.temp)/n,rho = lam*sqrt(log(d)/n))
P=res.glasso$wi
C=res.glasso$w
}
dd=norm(D-D.temp, "2")^2/norm(D.temp, "2")^2 + norm(P-P.temp, "2")^2/norm(P.temp, "2")^2
P.temp=P
D.temp=D
C.temp=C
if (k>maxit) break;
}
if(k>maxit){
print("The results don't converge")
}
return(list(mu=mu.temp, P=P.temp, C=C.temp, D=D.temp, total.iterations=k,d=dd))
}
#lyuou: function for lasso
lasso_fit = function(Sigma_Z,Sigma_ZY,Sigma_Y,n,lam=NULL,min_nz=3){
# eigen decomposition of Z
# the tuning parameter is lam*sqrt(log(d)/n)
# if lam is NULL, use CV.glmnet to determine tuning parameter
if(sum(abs(Sigma_ZY))==0){
return(list(beta_hat=t(t(Sigma_ZY)),lam.min=lam,sigma2e=1))
}
eig_Z = eigen(Sigma_Z)
Sigma_Z_inv = diag(sqrt(eig_Z$values))%*%t(eig_Z$vectors)%*%Sigma_Z
delta = diag(1/sqrt(eig_Z$values))%*%t(eig_Z$vectors)%*%t(t(Sigma_ZY))
# lasso for Sigma_Z_inv~delta
if(is.null(lam)){
res = cv.glmnet(Sigma_Z_inv,delta,grouped = FALSE)
lam.min = res$lambda[which.min(res$cvm[which(res$nzero>=min_nz)])+sum(res$nzero<min_nz)]
#lam.min = cv.glmnet(Sigma_Z_inv,delta,grouped = FALSE)$lambda.min
}else{
lam.min = lam
}
res = glmnet(Sigma_Z_inv,delta,lambda = lam.min)
beta_hat = res$beta
#sigma2e = deviance(res)
sigma2e = as.numeric((t(beta_hat)%*%Sigma_Z%*%beta_hat-t(beta_hat)%*%Sigma_ZY*2+Sigma_Y)*n/(n-res$df-1)/(res$nobs-res$df+1))/2
return(list(beta_hat=beta_hat,lam.min=lam.min,sigma2e=sigma2e))
}
#lyuou: function for debiased lasso and confidence interval
debiasd_lasso_fit = function(Sigma_Z,Sigma_ZY,beta_hat,sigma2e,n,d,lam=NULL,min_nz=3){
# the tuning parameter is lam*sqrt(log(d)/n)
# if lam is NULL, use CV.glmnet to determine tuning parameter
# de-biased lasso
# start from lasso estimator
beta_hat_debias = beta_hat
# store marginal variance
var_hat_debias = rep(0,d)
# store gamma and omega
gamma.tmp = matrix(0,d-1,d)
ttt.tmp = rep(0,d)
Omega.tmp = matrix(0,d,d)
# p-value
p_value_beta_tmp = rep(0,d)
for(j in 1:d){
# estimated Sigma_Z
res.tmp = lasso_fit(Sigma_Z[-j,-j],Sigma_Z[j,-j],Sigma_Z[j,j],lam)
gamma.tmp[,j] = res.tmp$beta_hat[,1]
# same tuning parameter
ttt.tmp[j] = Sigma_Z[j,j] + t(gamma.tmp[,j])%*%Sigma_Z[-j,-j]%*%gamma.tmp[,j] -
2*Sigma_Z[j,-j]%*%gamma.tmp[,j] + res.tmp$lam.min*sum(abs(gamma.tmp[,j]))
Omega.tmp[,j] = 1
Omega.tmp[-j,j] = -gamma.tmp[,j]
Omega.tmp[,j] = Omega.tmp[,j]/as.numeric(ttt.tmp[j])
# de-biased lasso estimator
beta_hat_debias[j] = beta_hat_debias[j] + t(Omega.tmp[,j])%*%Sigma_ZY -
t(Omega.tmp[,j])%*%Sigma_Z%*%beta_hat
# marginal variance
var_hat_debias[j] = sigma2e*t(Omega.tmp[,j])%*%Sigma_Z%*%Omega.tmp[,j]
# features with confidence interval excluding zero
p_value_beta_tmp[j] = 2*(1-pnorm(abs(beta_hat_debias[j])/
sqrt(var_hat_debias[j])))
}
return(list(beta_hat=beta_hat_debias,var_hat=var_hat_debias,p_value=p_value_beta_tmp))
}
model_fit = function(X,Y,A,tol=1e-3,maxit=1e3,lam_g=NULL,lam_l=NULL,min_nz=NULL){
n = dim(Y)[2] ## Y: 1*n row vector.
d = dim(A)[2]
p = dim(A)[1]
# the tuning parameter of glasso is lam_g*sqrt(log(d)/n)
# if lam_g is NULL, use CVglasso to determine tuning parameter
if(!is.null(lam_g)){
lam_g = lam_g*sqrt(log(d)/n)
}
# the tuning parameter of glmnet is lam_l*sqrt(log(d)/n)
# if lam_l is NULL, use CV.glmnet to determine tuning parameter
if(!is.null(lam_l)){
lam_l = lam_l*sqrt(log(d)/n)
}
# the tuning parameter of glasso is lam_g*sqrt(log(d)/n)
# if lam_g is NULL, use CVglasso to determine tuning parameter
if(!is.null(lam_g)){
lam_g = lam_g*sqrt(log(d)/n)
}
# minimum number of non zero entries inCV.glmnet
# if min_nz is NULL, let min_nz=max(p/10,3)
if(!is.null(min_nz)){
min_nz = max(floor(p/10),3)
}
# EM procedure
aa=em1(X,A,tol,maxit,n,d,lam_g)
Sigma_Z=solve(aa$P)
Sigma_ZY=solve(t(A)%*%A)%*%t(A)%*%rowMeans(X%*%diag(as.vector(Y)))
Sigma_Y=mean(as.vector(Y)^2)
# lasso
res_lasso = lasso_fit(Sigma_Z,Sigma_ZY,Sigma_Y,n,lam=lam_l,min_nz)
beta_hat = res_lasso$beta_hat
# debiased lasso
result_1 = debiasd_lasso_fit(Sigma_Z,Sigma_ZY,beta_hat,res_lasso$sigma2e,n,d,lam=lam_l,min_nz)
# p-value
p_value_beta = result_1$p_value
# prediction of y
Z = solve(t(A)%*%A)%*%t(A)%*%X
y_pred = t(result_1$beta_hat)%*%Z
return(list(p_value_beta=p_value_beta,y_pred=y_pred))
}
=20
i.rep=20
n=100
p=d=50
s=10
snr1=snr2=5
set.seed(2023)
# A is p by d binary matrix
A = matrix(rbinom(p*d,size = 1,prob = 0.5),p,d)
# Z is d by n sparse matrix
temp <- erdos.renyi.game(n = d, p = 0.3) # 30% of the entries are 0.
adj <- as_adjacency_matrix(temp, sparse = F)
tot_not0 <- sum(adj == 1)
sample_U <- runif(tot_not0, 0.2, 0.5)
Cg <- adj
Cg[adj == 1] <- sample_U
S_star <- (Cg + t(Cg))/2
# to make C_star positive definite
diag(S_star) <- 0.5 - eigen(S_star)$values[d]
# S_star is the covariance matrix of Z
C_star = solve(S_star)
# beta is sparse
beta=c(rep(1,s),rep(0,d-s))
# snr1 controls snr for linear model
sigmaepsilon2 = t(beta)%*%S_star%*%beta/snr1/5
# snr2 controls snr for factor model
# Lambda is the covariance matrix of u
Lambda = diag(diag(A%*%S_star%*%t(A))/snr2)/5
# seed from i.rep
set.seed(2023+i.rep)
Z=t(mvrnorm(n,rep(0,d),S_star))
# u ~ N(0,Lambda)
u=t(mvrnorm(n,rep(0,p),Lambda))
# epsilon ~ N(0,sigmaepsilon2)
epsilon=t(rnorm(n,0,sigmaepsilon2))
X=A%*%Z+u
Y=t(t(Z)%*%beta)+epsilon
n_fold=5
trainControl(method = "cv",number = n_fold)
library(caret)
crossv_kfold(mtcars, k = 5)
library(modelr)
crossv_kfold(mtcars, k = 5)
crossv_kfold(rbind(Y,X), k = n_fold)
?crossv_kfold
data.frame(rbind(Y,X)
)
crossv_kfold(data.frame(t(rbind(Y,X))), k = n_fold)
cv_split = crossv_kfold(data.frame(t(rbind(Y,X))), k = n_fold)
cv_split$train
cv_split$train[[1]]
i=1
cv_split$train$i
cv_split$1
cv_split$train
cv_split$train[[1]]
cv_split$train[1]
cv.glmnet()
cv.glmnet
model_fit = function(X,Y,A,tol=1e-3,maxit=1e3,lam_g=NULL,lam_l=NULL,min_nz=NULL){
n = dim(Y)[2] ## Y: 1*n row vector.
d = dim(A)[2]
p = dim(A)[1]
# the tuning parameter of glasso is lam_g*sqrt(log(d)/n)
# if lam_g is NULL, use CVglasso to determine tuning parameter
if(!is.null(lam_g)){
lam_g = lam_g*sqrt(log(d)/n)
}
# the tuning parameter of glmnet is lam_l*sqrt(log(d)/n)
# if lam_l is NULL, use CV.glmnet to determine tuning parameter
if(!is.null(lam_l)){
lam_l = lam_l*sqrt(log(d)/n)
}
# the tuning parameter of glasso is lam_g*sqrt(log(d)/n)
# if lam_g is NULL, use CVglasso to determine tuning parameter
if(!is.null(lam_g)){
lam_g = lam_g*sqrt(log(d)/n)
}
# minimum number of non zero entries inCV.glmnet
# if min_nz is NULL, let min_nz=max(p/10,3)
if(!is.null(min_nz)){
min_nz = max(floor(p/10),3)
}
# EM procedure
aa=em1(X,A,tol,maxit,n,d,lam_g)
Sigma_Z=solve(aa$P)
Sigma_ZY=solve(t(A)%*%A)%*%t(A)%*%rowMeans(X%*%diag(as.vector(Y)))
Sigma_Y=mean(as.vector(Y)^2)
# lasso
res_lasso = lasso_fit(Sigma_Z,Sigma_ZY,Sigma_Y,n,lam=lam_l,min_nz)
beta_hat = res_lasso$beta_hat
# debiased lasso
result_1 = debiasd_lasso_fit(Sigma_Z,Sigma_ZY,beta_hat,res_lasso$sigma2e,n,d,lam=lam_l,min_nz)
# p-value
p_value_beta = result_1$p_value
beta_hat = result_1$beta_hat
return(list(p_value_beta=p_value_beta,beta_hat=beta_hat))
}
model_fit = function(X,Y,A,tol=1e-3,maxit=1e3,lam_g=NULL,lam_l=NULL,min_nz=NULL){
n = dim(Y)[2] ## Y: 1*n row vector.
d = dim(A)[2]
p = dim(A)[1]
# the tuning parameter of glasso is lam_g*sqrt(log(d)/n)
# if lam_g is NULL, use CVglasso to determine tuning parameter
if(!is.null(lam_g)){
lam_g = lam_g*sqrt(log(d)/n)
}
# the tuning parameter of glmnet is lam_l*sqrt(log(d)/n)
# if lam_l is NULL, use CV.glmnet to determine tuning parameter
if(!is.null(lam_l)){
lam_l = lam_l*sqrt(log(d)/n)
}
# the tuning parameter of glasso is lam_g*sqrt(log(d)/n)
# if lam_g is NULL, use CVglasso to determine tuning parameter
if(!is.null(lam_g)){
lam_g = lam_g*sqrt(log(d)/n)
}
# minimum number of non zero entries inCV.glmnet
# if min_nz is NULL, let min_nz=max(p/10,3)
if(!is.null(min_nz)){
min_nz = max(floor(p/10),3)
}
# EM procedure
aa=em1(X,A,tol,maxit,n,d,lam_g)
Sigma_Z=solve(aa$P)
Sigma_ZY=solve(t(A)%*%A)%*%t(A)%*%rowMeans(X%*%diag(as.vector(Y)))
Sigma_Y=mean(as.vector(Y)^2)
# lasso
res_lasso = lasso_fit(Sigma_Z,Sigma_ZY,Sigma_Y,n,lam=lam_l,min_nz)
beta_hat = res_lasso$beta_hat
# debiased lasso
result_1 = debiasd_lasso_fit(Sigma_Z,Sigma_ZY,beta_hat,res_lasso$sigma2e,n,d,lam=lam_l,min_nz)
# p-value
p_value_beta = result_1$p_value
beta_hat_debias = result_1$beta_hat
return(list(p_value_beta=p_value_beta,beta_hat=beta_hat_debias))
}
cut(seq(1,nrow(yourData)),breaks=10,labels=FALSE)
cut(seq(1,nrow(Y)),breaks=n_fold,labels=FALSE)
seq(1,nrow(Y))
cut(seq(1,ncol(Y)),breaks=n_fold,labels=FALSE)
createFolds(y, k = 10, list = TRUE, returnTrain = FALSE)
library(caret)
setwd("C:/Dropbox/PARSE/paper files/simulation/parse-R-version0.1.1/PARSE")
library(devtools)
setwd("C:/Dropbox/PARSE/paper files/simulation")
install.packages("parse-R-version0.1.1/PARSE_0.1.1.tar.gz", repos=NULL, type="source")
library(PARSE)
library(snow)
library(rlecuyer) # loads the random number generator needed for multi-core
library(doParallel) # replace parallel from snow
library(foreach)
setwd("c:/Dropbox/PARSE/paper files/simulation/islet")
load(file = "log_islet.Rdata")
parse.fit = parse_large(K = 4, lambda = 5, y = y, kms.nstart = 10, cores = 8)
parse.fit
load(file = "log_normalized_islet.Rdata")
dim(y1)
save(y1,file = "islet.rda")
load("C:/Dropbox/Highd-CESME/CESME/simulations/code/cesme_test1111/data/buettner.rda")
dim(data_buettner)
setwd("C:/Dropbox/PARSE/paper files/simulation/parse-R-version0.1.1/PARSE")
library(devtools)
build_manual()
setwd("c:/Dropbox/Highd-CESME/CESME/simulations/code/cesme_test1111")
build_manual()
library(devtools)
document()
build()
build_manual()
setwd("c:/Dropbox/Highd-CESME/CESME/simulations/code/cesme_test1111")
document()
build()
build_manual()
setwd("c:/Dropbox/Highd-CESME/CESME/simulations/code/cesme_test1111")
setwd("c:/Dropbox/Highd-CESME/CESME/simulations/code/cesme_test1111")
document()
document()
build()
build_manual()
